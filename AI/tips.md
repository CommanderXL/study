* RNN (recurrent neutral network) 循环神经网络
* multi-head attention 多头注意
* masked 掩码
* BERT(bidirectional encoder representations from Transformers，基于Transformer的双向编码器表示)，它只使用编码器，完全移除了解码器；
* BERT 类模型通常用以迁移学习（transfer learning），这包括首先针对语言模型进行预训练（pretraining），然后针对特定任务进行微调（fine-tuning）。-> 仅编码器模型

大语言模型通过预测文本序列的下一个单词来进行预训练。

模型包含了 x 个参数，每个参数都是一个数值，代表了模型与语言的理解。

* foundation model 基础模型：预训练后的大语言模型；

* LLM 的训练范式：
  * 预训练：在大规模、多样化的数据集上进行训练，以形成全面的语言理解能力，即语言建模 -》具备了文本补全、少样本能力
  * 微调：在规模较小的特定任务或领域数据集上对模型进行针对性训练，进一步提升其特定能力（例如 ChatGPT 和 GPT-3 之间的关系）

* GPU 显存，在选择 GPU 时一个重要的考量因素是可用的 VRAM(video random access memory 视频随机存储器，即显存)的容量；不存在一种统一的规则可以确定一个模型具体需要多少显存容量，主要取决于模型的架构、规模、压缩技术、上下文长度、运行模型等后端的因素。


## 词元和嵌入

包括大模型在内的深度神经网络模型无法直接处理原始文本，由于文本数据是离散的，所以我们无法直接用它来执行神经网络训练所需的数学运算，我们需要将一种单词表示为连续值的向量格式的方法。

**嵌入：将数据转换为向量格式的过程。**嵌入的本质是将离散对象（如单词、图像甚至整个文档）映射到连续向量空间的点，其主要的目的是将非数值的数据转换为神经网络可以处理的格式。

词元不仅是模型的输出单位，也是模型查看输入的单位和方式。

* 深入了解分词器：Designing Large Language Model Applications
* 如果想更详细地了解分词器的训练，可以参考Hugging Face上的NLP课程中分词器相关的部分，以及Natural Language Processing with Transformers, Revised Edition一书

分词器（包含词元表、）的作用：

1. 除了把输入文本处理为语言模型的输入外，分词器还负责语言模型的输出，将生成的词元ID转换为与之关联的词元ID或词


词元嵌入：词元的数值表示，捕获了词元的含义；（向量表示）
文本嵌入：接受一段文本，最终生成单个向量，这个向量以某种形式表示该文本并捕捉其含义；

**词嵌入的维度（dimension）可以从一维到数千维不等，更高的维度有助于捕捉到更细微的关系，但这通常以牺牲计算效率为代价。**

## LLM 的内部机制

* 前向传播：在机器学习中，前向传播指的是输入进入神经网络并流经计算图，最终在另一端产生输出的计算过程；
* autoregressive model 自回归模型：将之前的输出作为未来预测的输入，例如模型使用第一个词元来生成第二个词元。
* 语言建模头（language modeling head）
* RoPE（rotary position embedding）旋转位置嵌入
* 涌现（emergence）：模型能够完成未经明确训练的任务的能力，这种能力并非模型在训练期间被明确教授所得，而是其广泛接触大量多语言数据和各种上文的自然结果。


### 注意力机制

* 自注意力：它通过允许一个序列中的每个位置与同一序列中的其他所有位置进行交互并权衡其重要性，来计算除更高效的输入表示。在自注意力机制中，“自”指的是该机制通过关联单个输入序列中的不同位置来计算注意力权重的能力。它可以评估并学习输入本身各个部分之间的关系和依赖，比如句子中的单词或者图像中的像素。 

在自注意力机制中，我们的目标**是为了输入序列中的每个元素计算上下文向量**。**上下文向量(context vector)**可以被理解为一种包含了序列中所有元素信息的嵌入向量。上下文向量在自注意力机制中起关键作用，它们的目的是通过集合序列当中其他所有元素的信息，为输入序列（如一个句子）中的每个元素创建丰富的表示，它是大模型理解句子中单词之间的关系和相关性。

训练过程中会产生3个投影矩阵，用于生成参与计算的组件：

* 查询投影矩阵；
* 键投影矩阵；
* 值投影矩阵；

1. 实现自注意力机制的第一步是计算中间值w，即所谓的**注意力分数**。（当前位置的查询向量与键矩阵相乘）

向量相似度的几种计算方式：

* 余弦相似度
* 欧式距离
* 点积

例如：点积越大，向量之间的对齐程度或者相似度就越高。在自注意力机制中，点积决定了序列中每个元素对其他元素的关注程度：点积越大，两个元素之间的相似度和注意力分数也就越高。

2. 接下来通过对于每个注意力分数进行归一化处理，来获得**总和为1的注意力权重**（注意力分数 到 注意力权重）。

3. 接下来通过将嵌入的输入词元x与相应的注意力权重相乘，再将的到的向量求和来计算上下文向量z。因此，上下文向量z是所有输入向量的加权总和，通过将每个输入向量与对应的注意力权重相乘而得。


因果注意力（也称为掩码注意力）是一种特殊的自注意力形式。它限制模型在处理任何给定词元时，只能基于序列中的先前和当前输入来计算注意力分数，而标准的自注意力机制可以一次性访问整个输入序列。


多头注意力(attention head)：“多头”是指将注意力机制分成多个“头”，每个“头”独立工作（每个头都有各自的投影矩阵）。在这种情况下，单个因果注意力模块可以被看作单头注意力，因为它只有一组注意力权重按顺序处理输入。在实际操作中，实现多头注意力需要构建多个自注意力机制的实例，**每个实例都有其独立的权重**，然后将这些输出进行合成。 -> 这提高了模型对于输入序列中复杂模式的建模能力，使其能够同时关注不同的模式。


### Transformer 架构

Transformer 块由以下两个首尾相接的组件构成：

* 自注意力层，主要负责整合来自其他输入词元和位置的相关信息（**向量数据**）；
* 前馈神经网络层，包含模型的主要处理能力；



## 文本分类

## 提示工程

* temperature 温度
* top-p 核采样


* 思维树(tree-of-thought)

需要注意的是，模型输出是否遵循指定的格式仍然取决于模型本身。有些模型比其他模型更善于遵循指令。


## 高级文本生成技术与工具

LangChain 最基础的链的形态是单链（单链架构），尽管链可以有多种形态和复杂度，但通常其表现为将 LLM 与补充工具、提示词模版或特定的功能相结合。


**我们通常需要在响应速度、记忆容量和准确率之间寻求平衡**。

两种广泛使用于对话记忆保持的方法：

### 对话缓冲区

* 一个是将过往的对话历史都作为提示词给到 llm，具备了记忆能力，但是很快会达到 llm 上下文窗口的限制；
* 窗口对话区：仅保留最后 n 轮对话的历史；

### 对话摘要

对完整对话记录进行摘要提炼，保留核心信息。提炼摘要的过程由另外一个 LLM 完成：该模型接收完整对话历史作为输入，并负责生成简明的摘要。这意味着每次向主 llm 发起查询时，系统会执行两次模型调用：

* 用户提示词处理；
* 摘要提示词生成；

通过提炼摘要可以有效压缩聊天记录长度，避免在 LLM 推理过程中消耗过多词元。不过存在两个局限：

* 原始问题信息未直接保留于聊天历史当中，模型需要依赖上下文进行推测；
* 系统需要对 LLM 进行两次的调用；


#### Agent

自主规划行动及其序列的系统被成为智能体，其核心在于利用语言模型自主制定行动决策。

智能体的核心机制：递进示推理